# Default use the NVIDIA official image with PyTorch 2.3.0
# https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html
ARG BASE_IMAGE=nvcr.io/nvidia/pytorch:24.02-py3
FROM ${BASE_IMAGE}

# Define environments
ENV MAX_JOBS=4
ENV FLASH_ATTENTION_FORCE_BUILD=TRUE
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

# Define installation arguments
ARG INSTALL_BNB=false
ARG INSTALL_VLLM=false
ARG INSTALL_DEEPSPEED=false
ARG INSTALL_FLASHATTN=false
ARG INSTALL_LIGER_KERNEL=false
ARG INSTALL_HQQ=false
ARG INSTALL_EETQ=false
ARG PIP_INDEX=https://pypi.org/simple
ARG HTTP_PROXY=
ARG NGINX=true
ARG API_MODEL_NAME="Qwen/Qwen2.5-1.5B-Instruct"

ENV FASTAPI_ROOT_PATH="/api/inference"
ENV API_MODEL_NAME=$API_MODEL_NAME
ENV MODEL_NAME=$API_MODEL_NAME

# CONSUL

WORKDIR /ops
ENV CONFIGDIR=/ops/shared/config
ENV CONSULVERSION=1.18.2
ENV VAULTVERSION=1.17.0
ENV NOMADVERSION=1.8.1
ENV CONSULTEMPLATEVERSION=0.39.0
ENV CONSULTEMPLATECONFIGDIR=/etc/consul-template.d
ENV CONSULTEMPLATEDIR=/opt/consul-template
ENV CONSULCONFIGDIR=/etc/consul.d


RUN apt-get -y update && apt-get -y upgrade && \
    apt-get install -y software-properties-common && \
    add-apt-repository universe && apt-get update && \
    apt-get install -y unzip tree redis-tools jq curl tmux && \
    apt-get clean


RUN mkdir -p $CONSULTEMPLATECONFIGDIR && \
 chmod 755 $CONSULTEMPLATECONFIGDIR && \
 mkdir -p $CONSULTEMPLATEDIR && \
 chmod 755 $CONSULTEMPLATEDIR

# Install HashiCorp Apt Repository
RUN wget -O- https://apt.releases.hashicorp.com/gpg /usr/share/keyrings/hashicorp-archive-keyring.gpg | gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg && \
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | tee /etc/apt/sources.list.d/hashicorp.list

# Install HashiStack Packages
RUN apt-get update && apt-get -y install \
	consul=$CONSULVERSION* \
	vault=$VAULTVERSION* \
	consul-template=$CONSULTEMPLATEVERSION*


# NGINX
RUN if [ "$NGINX" == "true" ]; then \
    	apt-get install -y nginx; \
	fi

COPY ./nginx/inference_nginx.conf /etc/nginx/sites-available/nginx.conf

# LLAMAFACTORY
WORKDIR /app

# Set http proxy
RUN if [ -n "$HTTP_PROXY" ]; then \
        echo "Configuring proxy..."; \
        export http_proxy=$HTTP_PROXY; \
        export https_proxy=$HTTP_PROXY; \
    fi

# Install the requirements
COPY requirements.txt /app
RUN pip config set global.index-url "$PIP_INDEX" && \
    pip config set global.extra-index-url "$PIP_INDEX" && \
    python -m pip install --upgrade pip && \
    if [ -n "$HTTP_PROXY" ]; then \
        python -m pip install --proxy=$HTTP_PROXY -r requirements.txt; \
    else \
        python -m pip install -r requirements.txt; \
    fi

# Copy the rest of the application into the image
COPY . /app

# Install the LLaMA Factory
RUN EXTRA_PACKAGES="metrics"; \
    if [ "$INSTALL_BNB" == "true" ]; then \
        EXTRA_PACKAGES="${EXTRA_PACKAGES},bitsandbytes"; \
    fi; \
    if [ "$INSTALL_VLLM" == "true" ]; then \
        EXTRA_PACKAGES="${EXTRA_PACKAGES},vllm"; \
    fi; \
    if [ "$INSTALL_DEEPSPEED" == "true" ]; then \
        EXTRA_PACKAGES="${EXTRA_PACKAGES},deepspeed"; \
    fi; \
    if [ "$INSTALL_LIGER_KERNEL" == "true" ]; then \
        EXTRA_PACKAGES="${EXTRA_PACKAGES},liger-kernel"; \
    fi; \
    if [ "$INSTALL_HQQ" == "true" ]; then \
        EXTRA_PACKAGES="${EXTRA_PACKAGES},hqq"; \
    fi; \
    if [ "$INSTALL_EETQ" == "true" ]; then \
        EXTRA_PACKAGES="${EXTRA_PACKAGES},eetq"; \
    fi; \
    if [ -n "$HTTP_PROXY" ]; then \
        pip install --proxy=$HTTP_PROXY -e ".[$EXTRA_PACKAGES]"; \
    else \
        pip install -e ".[$EXTRA_PACKAGES]"; \
    fi

# Rebuild flash attention
RUN pip uninstall -y transformer-engine flash-attn && \
    if [ "$INSTALL_FLASHATTN" == "true" ]; then \
        pip uninstall -y ninja && \
        if [ -n "$HTTP_PROXY" ]; then \
            pip install --proxy=$HTTP_PROXY ninja && \
            pip install --proxy=$HTTP_PROXY --no-cache-dir flash-attn --no-build-isolation; \
        else \
            pip install ninja && \
            pip install --no-cache-dir flash-attn --no-build-isolation; \
        fi; \
    fi


# Unset http proxy
RUN if [ -n "$HTTP_PROXY" ]; then \
        unset http_proxy; \
        unset https_proxy; \
    fi

# VOLUMES
VOLUME [ "/root/.cache/huggingface", "/root/.cache/modelscope", "/app/data", "/app/output" ]

# ENTRYPOINTS
COPY ./docker/entrypoints/entrypoint-inference.sh ./entrypoint.sh
ENTRYPOINT ["/bin/sh", "entrypoint.sh"]


# EXPOSE
EXPOSE 80
EXPOSE 8201 8200
EXPOSE 8300
EXPOSE 8301 8301/udp 8302 8302/udp
EXPOSE 8500 8600 8600/udp